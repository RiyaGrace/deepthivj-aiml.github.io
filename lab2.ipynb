{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 2-------29/05/2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happi'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TASK 2   STEMMING:\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmerporter = PorterStemmer()\n",
    "stemmerporter.stem('happiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmerLan =LancasterStemmer()\n",
    "stemmerLan.stem('happiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ing'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import RegexpStemmer\n",
    "stemmerregexp=RegexpStemmer('learn')\n",
    "stemmerregexp.stem('learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mang'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "SnowballStemmer.languages\n",
    "frenchstemmer=SnowballStemmer('french')\n",
    "frenchstemmer.stem('manges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Am quick brown fox jump over a lazi dog\n"
     ]
    }
   ],
   "source": [
    "#TASK 3: STEMMING PARAGRAPHS\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "example = \"Am quick brown fox jumps over a lazy dog\"\n",
    "example = [stemmer.stem(token) for token in example.split(\" \")]\n",
    "print (\" \".join(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cactus\n",
      "mouse\n",
      "rock\n",
      "good\n",
      "Am\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "# TASK 4: LEMMATIZER\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"mice\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "\n",
    "print(lemmatizer.lemmatize(\"better\", pos = 'a')) # given the part-of-speech, better lemmatizes to good\n",
    "\n",
    "print(lemmatizer.lemmatize(\"Am\"))   # This error is fixed when the PArt of Speech is given\n",
    "\n",
    "print(lemmatizer.lemmatize(\"am\", pos = 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"searching\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"searches\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocking\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"rocking\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"better\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"good\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"cars\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"car\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automobile\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"automobile\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rock'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmerLan.stem('rocking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmerLan.stem('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using LancasterStemmer\n",
    "import nltk \n",
    "from nltk.stem import LancasterStemmer \n",
    "stemmerLan =LancasterStemmer() \n",
    "stemmerLan.stem('working') \n",
    "stemmerLan.stem('happiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shut'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmerLan.stem('shutter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'but'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmerLan.stem('butter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmerLan.stem('working') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'swim'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using PorterStemmer\n",
    "import nltk \n",
    "from nltk.stem import PorterStemmer \n",
    "stemmerporter = PorterStemmer() \n",
    "stemmerporter.stem('swimming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weak'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmerporter.stem('weakness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weak'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.stem import RegexpStemmer \n",
    "stemmerregexp=RegexpStemmer('ness') \n",
    "stemmerregexp.stem('weakness') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\ACER\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 4.839 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "把 句子 中所 所有 的 可以 成 词 的 词语 都 扫描 描出 描出来 出来\n"
     ]
    }
   ],
   "source": [
    "# TASK 5: CHINESE SEGMENTATION USING JIEBA\n",
    "\n",
    "import jieba\n",
    "seg = jieba.cut(\"把句子中所有的可以成词的词语都扫描出来\", cut_all = True)\n",
    "print(\" \".join(seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\ACER\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 4.760 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私 は 幸 せに 暮 らしています\n"
     ]
    }
   ],
   "source": [
    "#with another sentence in japanese\n",
    "import jieba\n",
    "seg = jieba.cut(\"私は幸せに暮らしています\", cut_all = True)\n",
    "print(\" \".join(seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Become', 'an', 'expert', 'in', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "# TASK 6: BASIC TEXT PROCESSING PIPELINE\n",
    "\n",
    "import nltk\n",
    "sent = \"Become an expert in NLP\"\n",
    "words = nltk.word_tokenize(sent)\n",
    "print(words)\n",
    "\n",
    "texts = [\"\"\"The only true wisdom is in knowin' you know nothing.\n",
    "Beware the barrenness of a busy life.\n",
    "I decided that it was not wisdom that enabled poets to write their poetry,\n",
    "but a kind of ins. or inspiration, such as you find in seers and prophets who deliver all their sublime messages without knowing in the least what they mean. Be as you wish to seem. Wonder is the beginning of wisdom. Be kind, for everyone you meet is fighting a hard battle.  Our prayers should be for blessings in general, for God knows best what is good for us.\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'only', 'true', 'wisdom', 'is', 'in', 'knowin', \"'\", 'you', 'know', 'nothing', '.']\n",
      "[('The', 'DT'), ('only', 'JJ'), ('true', 'JJ'), ('wisdom', 'NN'), ('is', 'VBZ'), ('in', 'IN'), ('knowin', 'NN'), (\"'\", \"''\"), ('you', 'PRP'), ('know', 'VBP'), ('nothing', 'NN'), ('.', '.')]\n",
      "['Beware', 'the', 'barrenness', 'of', 'a', 'busy', 'life', '.']\n",
      "[('Beware', 'NNP'), ('the', 'DT'), ('barrenness', 'NN'), ('of', 'IN'), ('a', 'DT'), ('busy', 'JJ'), ('life', 'NN'), ('.', '.')]\n",
      "['I', 'decided', 'that', 'it', 'was', 'not', 'wisdom', 'that', 'enabled', 'poets', 'to', 'write', 'their', 'poetry', ',', 'but', 'a', 'kind', 'of', 'ins', '.']\n",
      "[('I', 'PRP'), ('decided', 'VBD'), ('that', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('not', 'RB'), ('wisdom', 'JJ'), ('that', 'IN'), ('enabled', 'VBD'), ('poets', 'NNS'), ('to', 'TO'), ('write', 'VB'), ('their', 'PRP$'), ('poetry', 'NN'), (',', ','), ('but', 'CC'), ('a', 'DT'), ('kind', 'NN'), ('of', 'IN'), ('ins', 'NNS'), ('.', '.')]\n",
      "['or', 'inspiration', ',', 'such', 'as', 'you', 'find', 'in', 'seers', 'and', 'prophets', 'who', 'deliver', 'all', 'their', 'sublime', 'messages', 'without', 'knowing', 'in', 'the', 'least', 'what', 'they', 'mean', '.']\n",
      "[('or', 'CC'), ('inspiration', 'NN'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('you', 'PRP'), ('find', 'VBP'), ('in', 'IN'), ('seers', 'NNS'), ('and', 'CC'), ('prophets', 'NNS'), ('who', 'WP'), ('deliver', 'VBP'), ('all', 'DT'), ('their', 'PRP$'), ('sublime', 'NN'), ('messages', 'NNS'), ('without', 'IN'), ('knowing', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('least', 'JJS'), ('what', 'WP'), ('they', 'PRP'), ('mean', 'VBP'), ('.', '.')]\n",
      "['Be', 'as', 'you', 'wish', 'to', 'seem', '.']\n",
      "[('Be', 'VB'), ('as', 'IN'), ('you', 'PRP'), ('wish', 'VBP'), ('to', 'TO'), ('seem', 'VB'), ('.', '.')]\n",
      "['Wonder', 'is', 'the', 'beginning', 'of', 'wisdom', '.']\n",
      "[('Wonder', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('beginning', 'NN'), ('of', 'IN'), ('wisdom', 'NN'), ('.', '.')]\n",
      "['Be', 'kind', ',', 'for', 'everyone', 'you', 'meet', 'is', 'fighting', 'a', 'hard', 'battle', '.']\n",
      "[('Be', 'NNP'), ('kind', 'NN'), (',', ','), ('for', 'IN'), ('everyone', 'NN'), ('you', 'PRP'), ('meet', 'VBP'), ('is', 'VBZ'), ('fighting', 'VBG'), ('a', 'DT'), ('hard', 'JJ'), ('battle', 'NN'), ('.', '.')]\n",
      "['Our', 'prayers', 'should', 'be', 'for', 'blessings', 'in', 'general', ',', 'for', 'God', 'knows', 'best', 'what', 'is', 'good', 'for', 'us', '.']\n",
      "[('Our', 'PRP$'), ('prayers', 'NNS'), ('should', 'MD'), ('be', 'VB'), ('for', 'IN'), ('blessings', 'NNS'), ('in', 'IN'), ('general', 'JJ'), (',', ','), ('for', 'IN'), ('God', 'NNP'), ('knows', 'VBZ'), ('best', 'JJS'), ('what', 'WP'), ('is', 'VBZ'), ('good', 'JJ'), ('for', 'IN'), ('us', 'PRP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#TOKENIZING AND POS-TAGGING\n",
    "for text in texts:\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        print(words)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        print(tagged)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
